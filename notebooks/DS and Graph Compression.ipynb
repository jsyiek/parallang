{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa3df97-7a59-4a1a-bff8-af47b6543a1b",
   "metadata": {},
   "source": [
    "# Path Visualizer\n",
    "\n",
    "## What is this?\n",
    "\n",
    "This notebook accomplishes the following:\n",
    "1) **Downloading** and **formatting** data in a form usable by the **Parallang Simulator**.\n",
    "2) **Visualization** of the true shortest route between **any two points** on the graph.\n",
    "3) **Reading** and **visualization** of the output of the **Parallang Simulator** for comparison.\n",
    "\n",
    "## How do I start?\n",
    "\n",
    "Begin by **installing and importing the below modules**. Then, read the instructions in each section carefully to find what needs to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb1d0a-7e5a-42cd-af4a-06aca41f707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install matplotlib\n",
    "!pip3 install osmnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53443118-4d7f-495d-9b05-385faee572a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c0039-09a7-425f-b510-9ed42e33edc5",
   "metadata": {},
   "source": [
    "## Section 1: Downloading of Data\n",
    "\n",
    "Set the below variables and run the rest of the blocks in this section before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0463f33-d4aa-4a36-8686-5e172aa53684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS: Useful if you need something specific.\n",
    "SOUTHERN_ENGLAND = ['Bristol, UK', 'Cornwall, UK', 'Devon, UK', 'Dorset, UK', 'Gloucestershire, UK', 'Somerset, UK', 'Wiltshire, UK', 'Berkshire, UK', 'Buckinghamshire, UK', 'West Sussex, UK', 'East Sussex, UK', 'Kent, UK', 'Oxfordshire, UK', 'Hampshire, UK', 'Isle of Wight, UK', 'Surrey, UK', 'City of London, UK', 'Greater London, UK', 'Bedfordshire, UK', 'Cambridgeshire, UK', 'Hertfordshire, UK', 'Essex, UK', 'Norfolk, UK', 'Suffolk, UK']\n",
    "\n",
    "\n",
    "# CHANGE ME: Location to get data from.\n",
    "\n",
    "place_name = \"Stroud, UK\"\n",
    "\n",
    "# Optional (not recommended to change): Change me if you want a different type of graph. Defaultly just paths you can drive.\n",
    "mode = None\n",
    "\n",
    "# Optional: It's a whitelist filter that filters out unwanted edges. For large queries, consider implementing a motorway filter\n",
    "# All others: custom_filter = '[\"highway\"~\"primary|primary_link|secondary|secondary_link|tertiary|tertiary_link\"]'\n",
    "# Southern England: '[\"highway\"~\"motorway\"]'\n",
    "# Sizes (post-compression):\n",
    "#    * Southern England (1351) <- 29795 (originally)\n",
    "#    * Gloucestershire (991)   <- 17941\n",
    "#    * Bristol (762)           <-  8479\n",
    "#    * Southampton (455)       <-  7188\n",
    "#    * Cheltenham (267)        <-  3941\n",
    "#    * Stroud (110)            <-  3570\n",
    "#    * Bishop's Cleeve (50)    <-   489\n",
    "custom_filter = '[\"highway\"~\"primary|primary_link|secondary|secondary_link|tertiary|tertiary_link\"]'\n",
    "\n",
    "# Optional: Change me if you want a directed graph\n",
    "directed = False\n",
    "\n",
    "# Optional: Whether to apply graph compression\n",
    "apply_graph_compression = True & (not directed)\n",
    "\n",
    "# Optional: Whether to truncate\n",
    "truncate_to = 400000\n",
    "\n",
    "# Optional: Number of datasets to generate\n",
    "num_datasets = 1\n",
    "\n",
    "# CHANGE ME: Location to store edge list in.\n",
    "filename = f\"analyzed_datasets/{place_name}\"\n",
    "\n",
    "# Optional: Whether to output verbosely.\n",
    "ox.settings.log_console = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016c26b-be53-41c1-b2a3-d8b249fef215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must set simplify to false, otherwise the simplification ratios of this technique will be terrible!\n",
    "graph = ox.graph_from_place(place_name, network_type=mode, simplify=not apply_graph_compression, custom_filter=custom_filter)\n",
    "if not directed:\n",
    "    graph = ox.utils_graph.get_undirected(graph)\n",
    "\n",
    "nodes, edges = ox.graph_to_gdfs(graph)\n",
    "\n",
    "print(\"Graph downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b800c2-ba7e-4647-8f2c-2e3179551dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = {}\n",
    "nextid = 0\n",
    "\n",
    "shortest_direct_distances = {}\n",
    "\n",
    "# length is in meters\n",
    "lengths = edges[[\"osmid\", \"length\"]]\n",
    "\n",
    "print(\"Formatting graph...\")\n",
    "for u, u_keyed_df in lengths.groupby(level=0):\n",
    "    for v, uv_keyed_df in u_keyed_df.groupby(level=1):\n",
    "        if u not in translations:\n",
    "            translations[u] = nextid\n",
    "            nextid += 1\n",
    "        if v not in translations:\n",
    "            translations[v] = nextid\n",
    "            nextid += 1\n",
    "\n",
    "        for (i, length) in enumerate(uv_keyed_df[\"length\"]):\n",
    "            shortest_direct_distances[(translations[u], translations[v])] = min(int(length*1000), shortest_direct_distances.get((u, v), float(\"inf\")))\n",
    "            if not directed:\n",
    "                shortest_direct_distances[(translations[v], translations[u])] = shortest_direct_distances[(translations[u], translations[v])]\n",
    "inverted_translations = {v: k for (k, v) in translations.items()}\n",
    "\n",
    "two_valency_nodes = set()\n",
    "if apply_graph_compression:\n",
    "    print(\"Beginning compression...\")\n",
    "    edge_list_dict = defaultdict(set)\n",
    "    for ((e1, e2), d) in shortest_direct_distances.items():\n",
    "        if e1 == e2: continue\n",
    "        edge_list_dict[e1].add(e2)\n",
    "        edge_list_dict[e2].add(e1)\n",
    "    edge_list_dict = {k: list(v) for (k, v) in edge_list_dict.items()}\n",
    "\n",
    "    # first, find the two valency nodes\n",
    "    for n in edge_list_dict:\n",
    "        if len(edge_list_dict[n]) == 2:\n",
    "            two_valency_nodes.add(n)\n",
    "\n",
    "    print(f\"Found {len(two_valency_nodes)} two valency nodes, finding paths...\")\n",
    "    collapsed_paths = []\n",
    "    i = 0\n",
    "    visited = set()\n",
    "    for n in two_valency_nodes:\n",
    "        if n in visited:\n",
    "            continue\n",
    "\n",
    "        left_search = []\n",
    "        right_search = []\n",
    "        \n",
    "        left_search = [n, edge_list_dict[n][0]]\n",
    "        right_search = [edge_list_dict[n][1]]\n",
    "\n",
    "        visited.add(n)\n",
    "        \n",
    "        for search in [left_search, right_search]:\n",
    "            two_in_search = any(2 in s for s in [left_search, right_search])\n",
    "            if not search: \n",
    "                continue\n",
    "            while search[-1] in two_valency_nodes and any(k not in visited for k in edge_list_dict[search[-1]]):\n",
    "                visited.add(search[-1])\n",
    "                k = edge_list_dict[search[-1]][0] \n",
    "                if k in visited:\n",
    "                    k = edge_list_dict[search[-1]][1]\n",
    "                \n",
    "                search.append(k)\n",
    "        collapsed_paths.append((list(reversed(right_search)) + left_search)[1:-1])\n",
    "\n",
    "    print(f\"Collapsed to {len(collapsed_paths)} paths, compressing them...\")\n",
    "    better_egress = {}\n",
    "    path_length = {}\n",
    "    node_to_path_id = {}\n",
    "    collapsed_paths_dict = {}\n",
    "    egresses_to_path_id = defaultdict(list)\n",
    "    path_id_to_egresses = {}\n",
    "    path_id = 0\n",
    "    for p in collapsed_paths:\n",
    "        # # don't resolve stand-alone cycles\n",
    "        # if edge_list_dict[p[-1]] in edge_list_dict[p[0]]:\n",
    "        #     continue\n",
    "        \n",
    "        collapsed_paths_dict[path_id] = p\n",
    "        k = edge_list_dict[p[0]][0]\n",
    "        if k in visited: \n",
    "            k = edge_list_dict[p[0]][1]\n",
    "        j = edge_list_dict[p[-1]][0]\n",
    "        # some paths will be one node.\n",
    "        # in such a scenario, you need to \n",
    "        # ensure differing endpoints are used\n",
    "        # as p[-1] and p[0] will be the same\n",
    "        # and hence the len(p) == 1 check\n",
    "        if j in visited or len(p) == 1:\n",
    "            j = edge_list_dict[p[-1]][1]\n",
    "\n",
    "        egresses_to_path_id[(k, j)].append(path_id)\n",
    "        path_id_to_egresses[path_id] = (k, j)\n",
    "        \n",
    "        for n in p:\n",
    "            node_to_path_id[n] = path_id\n",
    "\n",
    "        length = sum(shortest_direct_distances[(a, b)] for (a, b) in zip(p[:-1], p[1:]))\n",
    "        right_escape = length + shortest_direct_distances[(p[-1], j)]\n",
    "        left_escape = shortest_direct_distances[(k, p[0])]\n",
    "        for (a, b) in zip(p[:-1], p[1:]):\n",
    "            if left_escape < right_escape:\n",
    "                better_egress[a] = (k, left_escape)\n",
    "            else:\n",
    "                better_egress[a] = (j, right_escape)\n",
    "            left_escape += shortest_direct_distances[(a, b)]\n",
    "            right_escape -= shortest_direct_distances[(a, b)]\n",
    "        if left_escape < right_escape:\n",
    "            better_egress[p[-1]] = (k, left_escape)\n",
    "        else:\n",
    "            better_egress[p[-1]] = (j, right_escape)\n",
    "        shortest_direct_distances[(k, j)] = shortest_direct_distances[(j, k)] = min(shortest_direct_distances.get((k, j), float(\"inf\")), right_escape + left_escape)\n",
    "        path_length[path_id] = right_escape + left_escape\n",
    "        path_id += 1\n",
    "    print(f\"Shuffling down vertex numbers to account for removed nodes...\")\n",
    "    compressed_shortest_direct_distances = {}\n",
    "    compressed_translations = {}\n",
    "    inverted_compressed_translations = {}\n",
    "    node_id = 0\n",
    "    for ((e1, e2), d) in shortest_direct_distances.items():\n",
    "        if e1 in visited or e2 in visited:\n",
    "            continue\n",
    "        for e in (e1, e2):\n",
    "            if e not in inverted_compressed_translations:\n",
    "                compressed_translations[node_id] = e\n",
    "                inverted_compressed_translations[e] = node_id\n",
    "                node_id += 1\n",
    "        e1_new = inverted_compressed_translations[e1]\n",
    "        e2_new = inverted_compressed_translations[e2]\n",
    "        compressed_shortest_direct_distances[(e1_new, e2_new)] = d\n",
    "        compressed_shortest_direct_distances[(e2_new, e1_new)] = d\n",
    "\n",
    "    # de-default-ify\n",
    "    egresses_to_path_id = {k : v for (k, v) in egresses_to_path_id.items()}\n",
    "\n",
    "dist_to_use = shortest_direct_distances if not apply_graph_compression else compressed_shortest_direct_distances\n",
    "\n",
    "max_node_num = max(max(shortest_direct_distances, key=lambda t: max(t)))\n",
    "max_node_num_compressed = max_node_num\n",
    "if apply_graph_compression:\n",
    "    max_node_num_compressed = max(max(compressed_shortest_direct_distances, key=lambda t: max(t)))\n",
    "\n",
    "for i in range(num_datasets):\n",
    "    old_dist_to_use = dist_to_use\n",
    "    if max_node_num_compressed > truncate_to:\n",
    "        print(f\"Going to truncate graph to {truncate_to}, currently {max_node_num_compressed+1} nodes\")\n",
    "        all_edges = (list(dist_to_use.items()) + list(((y, x), d) for ((x, y), d) in dist_to_use.items()))\n",
    "        random.shuffle(all_edges)\n",
    "        edge = random.choice(all_edges)\n",
    "        nodes = set((edge[0][0], edge[0][1]))\n",
    "        while len(nodes) < truncate_to: \n",
    "            for ((v1, v2), dist) in all_edges:\n",
    "                if v1 in nodes:\n",
    "                    nodes.add(v2)\n",
    "                if len(nodes) >= truncate_to:\n",
    "                    break\n",
    "        mappings = {v: i for (i, v) in enumerate(nodes)}\n",
    "        dist_to_use = {(mappings[v1], mappings[v2]) : d for ((v1, v2), d) in dist_to_use.items() if v1 in nodes and v2 in nodes}\n",
    "        print(\"Dropped\", len(all_edges) - len(dist_to_use), \"edges\")\n",
    "\n",
    "    filename = f\"analyzed_datasets/{place_name}Trunc{truncate_to}_{i}.points\"\n",
    "    with open(filename, \"w+\") as F:\n",
    "        drops = 0\n",
    "        for i, (k, v) in enumerate(dist_to_use.items()):\n",
    "            F.write(f\"{i} {k[0]} {k[1]} {v}\\n\")\n",
    "    dist_to_use = old_dist_to_use\n",
    "print(f\"The graph has been saved to {os.path.join(os.getcwd(), filename)}\") \n",
    "if apply_graph_compression:\n",
    "    # max_node_num_compressed = max(max(compressed_shortest_direct_distances, key=lambda t: max(t)))\n",
    "    # max_node_num = max(max(shortest_direct_distances, key=lambda t: max(t)))\n",
    "    node_compression_factor = round((max_node_num_compressed+1)/(max_node_num+1) * 100, 2)\n",
    "    edge_compression_factor = round(len(compressed_shortest_direct_distances)/len(shortest_direct_distances)*100, 2)\n",
    "    print(f\"Graph edge compression factor: {edge_compression_factor}%\")\n",
    "    print(f\"Graph node compression factor: {node_compression_factor}%\")\n",
    "    print(f\"Final graph size: {min(max_node_num_compressed, truncate_to)}. Originally: {max_node_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95121059-1c83-474c-97e8-da41291264a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = ox.plot_graph(graph)\n",
    "print(f\"Visualization of {place_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e9b92-036f-4d51-a886-2a482dc4a0ac",
   "metadata": {},
   "source": [
    "## Section 2: Loading Simulator Output\n",
    "\n",
    "Before progressing to this section, **run the simulator** on the output `.points` file. You can use the `EdgesReader` utility to load it into the system, and the `MatrixDumper` utility to dump the final results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548482e5-61fc-4eb8-b904-65e87f85bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ME: Path to successor/predecessor matrix.\n",
    "path_to_distance_matrix = \"distances.txt\"\n",
    "\n",
    "# CHANGE ME: Choose either the successor method or predecessor method depending on the underlying algorithm\n",
    "SUCCESSOR = \"successor\"\n",
    "PREDECESSOR = \"predecessor\"\n",
    "method = PREDECESSOR\n",
    "\n",
    "path_to_matrix = f\"{method}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad139c-42fc-4a05-aa22-9ac18234a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseMatrix(filepath: str, local_id_to_global_id=None, do_optimization=True):\n",
    "    with open(filepath, \"r\") as F:\n",
    "        lines = F.readlines()\n",
    "    lines = map(lambda a: map(int, a), map(lambda s: s.split(\" \"), lines))\n",
    "    output = []\n",
    "    for i, l in enumerate(lines):\n",
    "        if do_optimization and i not in local_id_to_global_id:\n",
    "            continue\n",
    "        translated_line = []\n",
    "        for e in l:\n",
    "            if do_optimization and e not in local_id_to_global_id:\n",
    "                continue\n",
    "            translated_line.append(e)\n",
    "        output.append(translated_line)\n",
    "\n",
    "    return output\n",
    "\n",
    "def safe_localize(node_l):\n",
    "    \"\"\"\n",
    "    safely resolves a global ID to a local one with graph compression\n",
    "    \"\"\"\n",
    "    if apply_graph_compression:\n",
    "        node_l = inverted_compressed_translations[node_l]\n",
    "    return node_l\n",
    "\n",
    "def path_from_successor(successor, start_l, fin_l):\n",
    "    start_l = safe_localize(start_l)\n",
    "    fin_l = safe_localize(fin_l)\n",
    "\n",
    "    output = [start_l]\n",
    "    prev_l = fin_l\n",
    "    while successor[output[-1]][fin_l] != fin_l:\n",
    "        if successor[output[-1]][fin_l] == output[-1]:\n",
    "            return []\n",
    "        output.append(successor[output[-1]][fin_l])\n",
    "    output.append(fin_l)\n",
    "\n",
    "    return list(map(lambda e: compressed_translations[e],  output))\n",
    "    # return list(map(lambda e: l2g_ids[e], output))\n",
    "\n",
    "def path_from_predecessor(predecessor, start_l, fin_l):\n",
    "    start_l = safe_localize(start_l)\n",
    "    fin_l = safe_localize(fin_l)\n",
    "    \n",
    "    output = [fin_l]\n",
    "    prev_l = fin_l\n",
    "    while predecessor[start_l][prev_l] != start_l:\n",
    "        if prev_l == predecessor[start_l][prev_l]:\n",
    "            return []\n",
    "        output.append(predecessor[start_l][prev_l])\n",
    "        prev_l = predecessor[start_l][prev_l]\n",
    "\n",
    "    output.append(start_l)\n",
    "    output.reverse()\n",
    "    return list(map(lambda e: compressed_translations[e],  output))\n",
    "    # return list(map(lambda e: l2g_ids[e], output))\n",
    "\n",
    "def calculatePath(matrix, distance_matrix, start_g, fin_g, g2l_ids, l2g_ids):\n",
    "    #print(\"Entry.\")\n",
    "    if start_g == fin_g:\n",
    "        return [start_g]\n",
    "    strategy = path_from_successor if method == SUCCESSOR else path_from_predecessor\n",
    "    start_l = g2l_ids[start_g]\n",
    "    fin_l = g2l_ids[fin_g]\n",
    "    start_c = None\n",
    "    fin_c = None\n",
    "    if start_l in inverted_compressed_translations:\n",
    "        start_c = inverted_compressed_translations[start_l]\n",
    "    if fin_l in inverted_compressed_translations:\n",
    "        fin_c = inverted_compressed_translations[fin_l]\n",
    "    \n",
    "    if not apply_graph_compression:\n",
    "        output = strategy(matrix, start_l, fin_l)\n",
    "        return list(map(lambda e: l2g_ids[e], output))\n",
    "\n",
    "    def ddash_within_path(a, b, pid):\n",
    "        (a_egg, a_dist) = better_egress[a]\n",
    "        (b_egg, b_dist) = better_egress[b]\n",
    "        if a_egg == b_egg:\n",
    "            return abs(a_dist - b_dist)\n",
    "        return abs((path_length[pid] - a_dist) - b_dist)\n",
    "\n",
    "    def ddash(a, egg, pid):\n",
    "        if a not in better_egress:\n",
    "            a, egg = egg, a\n",
    "        (a_egg, a_dist) = better_egress[a]\n",
    "        if a_egg == egg:\n",
    "            return a_dist\n",
    "        return abs((path_length[pid] - a_dist))\n",
    "        \n",
    "\n",
    "    def dc(a, b):\n",
    "        return distance_matrix[a][b]\n",
    "    \n",
    "    # case 1: neither vertex removed\n",
    "    if start_l in inverted_compressed_translations and fin_l in inverted_compressed_translations:\n",
    "        #print(\"Case 1.\")\n",
    "        path = strategy(matrix, start_l, fin_l)\n",
    "        output = []\n",
    "        for (v1, v2) in zip(path[:-1], path[1:]):\n",
    "            output.append(v1)\n",
    "            if (v1, v2) in egresses_to_path_id or (v2, v1) in egresses_to_path_id:\n",
    "                f = lambda l: l\n",
    "                pathlist = []\n",
    "                if (v2, v1) in egresses_to_path_id and (v1, v2) not in egresses_to_path_id:\n",
    "                    f = lambda l: reversed(l)\n",
    "                    pathlist = egresses_to_path_id[(v2, v1)]\n",
    "                else:\n",
    "                    pathlist = egresses_to_path_id[(v1, v2)]\n",
    "                best_path = None\n",
    "                best_dist = float(\"inf\")\n",
    "                for p in pathlist:\n",
    "                    if path_length[p] < best_dist:\n",
    "                        best_path = collapsed_paths_dict[p]\n",
    "                        best_dist = path_length[p]\n",
    "                output += list(f(best_path))\n",
    "        output.append(v2)\n",
    "        return list(map(lambda e: l2g_ids[e], output))\n",
    "                \n",
    "    # case 2: both removed, on same path\n",
    "    if start_l in node_to_path_id and fin_l in node_to_path_id and node_to_path_id[start_l] == node_to_path_id[fin_l]:\n",
    "        #print(\"Case 2.\")\n",
    "        pid = node_to_path_id[fin_l]\n",
    "        (s1, a_dist) = better_egress[start_l]\n",
    "        (s2, b_dist) = better_egress[fin_l]\n",
    "        s1c = inverted_compressed_translations[s1]\n",
    "        s2c = inverted_compressed_translations[s2]\n",
    "        candidate1 = ddash_within_path(start_l, fin_l, pid)\n",
    "        candidate2 = ddash(start_l, s1, pid) + dc(s1c, s2c) + ddash(s2, fin_l, pid)\n",
    "        path = collapsed_paths_dict[pid]\n",
    "        if candidate1 > candidate2:\n",
    "            if path.index(start_l) < path.index(fin_l):\n",
    "                path = list(reversed(path))\n",
    "            first_portion = path[path.index(start_l):]\n",
    "            last_portion = path[:path.index(fin_l)+1]\n",
    "            first_portion = list(map(lambda e: l2g_ids[e], first_portion))\n",
    "            last_portion = list(map(lambda e: l2g_ids[e], last_portion))\n",
    "            return first_portion + calculatePath(matrix, distance_matrix, inverted_translations[s1], inverted_translations[s2], g2l_ids, l2g_ids) + last_portion\n",
    "        else:\n",
    "            if path.index(start_l) > path.index(fin_l):\n",
    "                path = list(reversed(path))\n",
    "            output = path[path.index(start_l):path.index(fin_l)+1]\n",
    "            return list(map(lambda e: l2g_ids[e], output))\n",
    "\n",
    "    # case 3a: start removed, but end is not\n",
    "    if start_l in node_to_path_id and fin_l not in node_to_path_id:\n",
    "        print(\"Case 3a.\")\n",
    "        pid = node_to_path_id[start_l]\n",
    "        (s1, s2) = path_id_to_egresses[pid]\n",
    "        s1c = inverted_compressed_translations[s1]\n",
    "        s2c = inverted_compressed_translations[s2]\n",
    "        candidate1 = ddash(start_l, s1, pid) + dc(s1c, fin_c)\n",
    "        candidate2 = ddash(start_l, s2, pid) + dc(s2c, fin_c)\n",
    "\n",
    "        if candidate1 < candidate2:\n",
    "            egress_begin = s1\n",
    "        else:\n",
    "            egress_begin = s2\n",
    "\n",
    "        path = [s1] + collapsed_paths_dict[pid] + [s2]\n",
    "        if path.index(start_l) > path.index(egress_begin):\n",
    "            path = list(reversed(path))\n",
    "        first_portion = path[path.index(start_l):-1]\n",
    "        first_portion = list(map(lambda e: l2g_ids[e], first_portion))\n",
    "        return first_portion + calculatePath(matrix, distance_matrix, inverted_translations[egress_begin], fin_g, g2l_ids, l2g_ids)\n",
    "\n",
    "    # case 3b: start not removed, but end is \n",
    "    if start_l not in node_to_path_id and fin_l in node_to_path_id:\n",
    "        #print(\"Case 3b.\")\n",
    "        pid = node_to_path_id[fin_l]\n",
    "        (s1, s2) = path_id_to_egresses[pid]\n",
    "        s1c = inverted_compressed_translations[s1]\n",
    "        s2c = inverted_compressed_translations[s2]\n",
    "        candidate1 = dc(start_c, s1c) + ddash(s1, fin_l, pid)\n",
    "        candidate2 = dc(start_c, s2c) + ddash(s2, fin_l, pid)\n",
    "\n",
    "        if candidate1 < candidate2:\n",
    "            egress_end = s1\n",
    "        else:\n",
    "            egress_end = s2\n",
    "\n",
    "        path = [s1] + collapsed_paths_dict[pid] + [s2]\n",
    "        if path.index(fin_l) < path.index(egress_end):\n",
    "            path = list(reversed(path))\n",
    "        last_portion = path[1:path.index(fin_l)+1]\n",
    "        last_portion = list(map(lambda e: l2g_ids[e], last_portion))\n",
    "        return calculatePath(matrix, distance_matrix, start_g, inverted_translations[egress_end], g2l_ids, l2g_ids) + last_portion\n",
    "\n",
    "    # case 4: both removed but on different chains\n",
    "    #print(\"Case 4.\")\n",
    "    pid_start = node_to_path_id[start_l]\n",
    "    pid_fin = node_to_path_id[fin_l]\n",
    "\n",
    "    (s1_start, s2_start) = path_id_to_egresses[pid_start]\n",
    "    s1c_start = inverted_compressed_translations[s1_start]\n",
    "    s2c_start = inverted_compressed_translations[s2_start]\n",
    "\n",
    "    (s1_fin, s2_fin) = path_id_to_egresses[pid_fin]\n",
    "    s1c_fin = inverted_compressed_translations[s1_fin]\n",
    "    s2c_fin = inverted_compressed_translations[s2_fin]\n",
    "\n",
    "    candidate1 = ddash(start_l, s1_start, pid_start) + dc(s1c_start, s1c_fin) + ddash(s1_fin, fin_l, pid_fin)\n",
    "    candidate2 = ddash(start_l, s2_start, pid_start) + dc(s2c_start, s1c_fin) + ddash(s1_fin, fin_l, pid_fin)\n",
    "    candidate3 = ddash(start_l, s1_start, pid_start) + dc(s1c_start, s2c_fin) + ddash(s2_fin, fin_l, pid_fin)\n",
    "    candidate4 = ddash(start_l, s2_start, pid_start) + dc(s2c_start, s2c_fin) + ddash(s2_fin, fin_l, pid_fin)\n",
    "\n",
    "    best = min([candidate1, candidate2, candidate3, candidate4])\n",
    "    #print(best)\n",
    "    if best == candidate1:\n",
    "        s_start = s1_start\n",
    "        s_fin = s1_fin\n",
    "    elif best == candidate2:\n",
    "        s_start = s2_start\n",
    "        s_fin = s1_fin\n",
    "    elif best == candidate3:\n",
    "        s_start = s1_start\n",
    "        s_fin = s2_fin\n",
    "    else: # best == candidate4\n",
    "        s_start = s2_start\n",
    "        s_fin = s2_fin\n",
    "    \n",
    "    start_path = [s1_start] + collapsed_paths_dict[pid_start] + [s2_start]\n",
    "    if start_path.index(start_l) > start_path.index(s_start):\n",
    "        start_path = list(reversed(start_path))\n",
    "    first_portion = start_path[start_path.index(start_l):-1]\n",
    "    first_portion = list(map(lambda e: l2g_ids[e], first_portion))\n",
    "\n",
    "    fin_path = [s1_fin] + collapsed_paths_dict[pid_fin] + [s2_fin]\n",
    "    if fin_path.index(fin_l) < fin_path.index(s_fin):\n",
    "        fin_path = list(reversed(fin_path))\n",
    "    last_portion = fin_path[1:fin_path.index(fin_l)+1]\n",
    "    last_portion = list(map(lambda e: l2g_ids[e], last_portion))\n",
    "\n",
    "    return first_portion + \\\n",
    "        calculatePath(matrix, distance_matrix, inverted_translations[s_start], inverted_translations[s_fin], g2l_ids, l2g_ids) + \\\n",
    "        last_portion\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef49ab7-8a35-45c6-a350-0e9d75b8cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = parseMatrix(path_to_matrix, inverted_translations)\n",
    "distance_matrix = parseMatrix(path_to_distance_matrix, do_optimization=False)\n",
    "print(\"Simulator data loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac572b-4771-4ee2-abbb-af386fa76735",
   "metadata": {},
   "source": [
    "## Section 3: Comparison of Outputs\n",
    "\n",
    "To visualize paths, choose two nodes below and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c97b2-2ae6-4441-ab23-10b033c7ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "two_valency_nodes_l = list(two_valency_nodes)\n",
    "two_valency_nodes_g = list(map(lambda e: inverted_translations[e], two_valency_nodes_l))\n",
    "\n",
    "# CHANGE ME\n",
    "orig1 = list(graph.nodes())[0]\n",
    "dest1 = list(graph.nodes())[-1]\n",
    "orig2 = list(graph.nodes())[50]\n",
    "dest2 = list(graph.nodes())[-50]  \n",
    "orig3 = random.choice(two_valency_nodes_g) # 21272693\n",
    "dest3 = random.choice(two_valency_nodes_g) # 21272694\n",
    "orig4 = inverted_translations[20] #random.choice(list(graph.nodes()))\n",
    "dest4 = inverted_translations[27] #random.choice(list(graph.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb47dd-f9ee-4a1f-b786-7012eb941a72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate shortest paths for the 2 routes\n",
    "route1 = nx.shortest_path(graph, orig1, dest1, weight='length')\n",
    "route2 = nx.shortest_path(graph, orig2, dest2, weight='length')\n",
    "route3 = nx.shortest_path(graph, orig3, dest3, weight='length')\n",
    "route4 = nx.shortest_path(graph, orig4, dest4, weight='length')\n",
    "\n",
    "routes = [route1, route2, route3, route4]\n",
    "rc = ['b', 'y', 'g', 'r']\n",
    "fig, ax = ox.plot_graph_routes(graph, routes, route_colors=rc, route_linewidth=6, node_size=0)\n",
    "print(\"True shortest paths\")\n",
    "\n",
    "pt = ox.graph_to_gdfs(graph, edges=False).unary_union.centroid\n",
    "bbox = ox.utils_geo.bbox_from_point((pt.y, pt.x), dist=5000)\n",
    "#fig, ax = ox.plot_graph_routes(graph, [route4, route4], ['r', 'r'], bbox=bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1df50-2691-42e1-ade4-6feb445a4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "calcr1 = calculatePath(matrix, distance_matrix, orig1, dest1, translations, inverted_translations)\n",
    "calcr2 = calculatePath(matrix, distance_matrix, orig2, dest2, translations, inverted_translations)\n",
    "calcr3 = calculatePath(matrix, distance_matrix, orig3, dest3, translations, inverted_translations)\n",
    "calcr4 = calculatePath(matrix, distance_matrix, orig4, dest4, translations, inverted_translations)\n",
    "\n",
    "fig, ax = ox.plot_graph_routes(graph, [calcr1, calcr2, calcr3, calcr4], route_colors=rc, route_linewidth=6, node_size=0)\n",
    "print(\"Simulator shortest paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e677044-6203-428a-b3b9-a1a2d770a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Do they agree on path 1? \" + \"Yes!\" if calcr1 == route1 else f\"No: \\n{calcr1} \\n{route1}\")\n",
    "print(\"Do they agree on path 2? \" + \"Yes!\" if calcr2 == route2 else f\"No: \\n{calcr2} \\n{route2}\")\n",
    "print(\"Do they agree on path 3? \" + \"Yes!\" if calcr3 == route3 else f\"No: \\n{calcr3} \\n{route3}\")\n",
    "print(\"Do they agree on path 4? \" + \"Yes!\" if calcr4 == route4 else f\"No: \\n{calcr4} \\n{route4}\")\n",
    "\n",
    "# nx.shortest_path_length(graph, orig2, dest2, weight='length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177539b-7484-468e-9021-b26054748df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(graph.nodes())\n",
    "for start in nodes:\n",
    "    for end in nodes:\n",
    "        route = nx.shortest_path(graph, start, end, weight='length')\n",
    "        try:\n",
    "            calcr = calculatePath(matrix, distance_matrix, start, end, translations, inverted_translations)\n",
    "        except BaseException as e:\n",
    "            print(start)\n",
    "            print(end)\n",
    "            \n",
    "            raise e\n",
    "        try:\n",
    "            assert(route == calcr)\n",
    "        except AssertionError as e:\n",
    "            # IMPORTANT NOTE: Any failed cases require manual investigation. The model solution will sometimes differ if there are multiple shortest paths.\n",
    "            #                 This is rare, but does occurs thanks to the many roundabouts in Britain. \n",
    "            print(\"Consider checking\", start, end)\n",
    "print(\"Test successful!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
